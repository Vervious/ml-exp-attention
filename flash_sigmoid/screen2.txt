

torch.__version__  = 2.6.0.dev20241225+cu126


running install
/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.
  warnings.warn(
/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/setuptools/command/easy_install.py:158: EasyInstallDeprecationWarning: easy_install command is deprecated. Use build and pip and other standards-based tools.
  warnings.warn(
running bdist_egg
running egg_info
writing flash_exp.egg-info/PKG-INFO
writing dependency_links to flash_exp.egg-info/dependency_links.txt
writing requirements to flash_exp.egg-info/requires.txt
writing top-level names to flash_exp.egg-info/top_level.txt
reading manifest file 'flash_exp.egg-info/SOURCES.txt'
reading manifest template 'MANIFEST.in'
warning: no files found matching '*.cu' under directory 'flash_exp'
warning: no files found matching '*.h' under directory 'flash_exp'
warning: no files found matching '*.cuh' under directory 'flash_exp'
warning: no files found matching '*.cpp' under directory 'flash_exp'
warning: no files found matching '*.hpp' under directory 'flash_exp'
adding license file 'LICENSE'
adding license file 'AUTHORS'
writing manifest file 'flash_exp.egg-info/SOURCES.txt'
installing library code to build/bdist.linux-aarch64/egg
running install_lib
running build_py
running build_ext
/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/utils/cpp_extension.py:456: UserWarning: The detected CUDA version (12.8) has a minor version mismatch with the version that was used to compile PyTorch (12.6). Most likely this shouldn't be a problem.
  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))
/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/utils/cpp_extension.py:466: UserWarning: There are no aarch64-linux-gnu-g++ version bounds defined for CUDA version 12.8
  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')
building 'flash_exp_2_cuda' extension
Emitting ninja build file /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/build.ninja...
Compiling objects...
Using envvar MAX_JOBS (64) as the number of workers...
[1/48] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_fwd_split_hdim224_fp16_sm80.o.d -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/cutlass/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/ben2/.venv/include -I/usr/include/python3.10 -c -c /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_fwd_split_hdim224_fp16_sm80.cu -o /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_fwd_split_hdim224_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1016"' -DTORCH_EXTENSION_NAME=flash_exp_2_cuda -D_GLIBCXX_USE_CXX11_ABI=1
[2/48] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_fwd_split_hdim256_fp16_sm80.o.d -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/cutlass/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/ben2/.venv/include -I/usr/include/python3.10 -c -c /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_fwd_split_hdim256_fp16_sm80.cu -o /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_fwd_split_hdim256_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1016"' -DTORCH_EXTENSION_NAME=flash_exp_2_cuda -D_GLIBCXX_USE_CXX11_ABI=1
[3/48] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim160_fp16_sm80.o.d -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/cutlass/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/ben2/.venv/include -I/usr/include/python3.10 -c -c /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim160_fp16_sm80.cu -o /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim160_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1016"' -DTORCH_EXTENSION_NAME=flash_exp_2_cuda -D_GLIBCXX_USE_CXX11_ABI=1
FAILED: /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim160_fp16_sm80.o 
/usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim160_fp16_sm80.o.d -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/cutlass/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/ben2/.venv/include -I/usr/include/python3.10 -c -c /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim160_fp16_sm80.cu -o /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim160_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1016"' -DTORCH_EXTENSION_NAME=flash_exp_2_cuda -D_GLIBCXX_USE_CXX11_ABI=1
/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=true]" at line 245 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim160<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim160_fp16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=true]" at line 245 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim160<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim160_fp16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=false]" at line 245 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim160<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim160_fp16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=false]" at line 245 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim160<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim160_fp16_sm80.cu

4 errors detected in the compilation of "/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim160_fp16_sm80.cu".
/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=true]" at line 245 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim160<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim160_fp16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=true]" at line 245 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim160<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim160_fp16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=false]" at line 245 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim160<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim160_fp16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::half_t>>, Is_dropout=false]" at line 245 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim160<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim160_fp16_sm80.cu

4 errors detected in the compilation of "/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim160_fp16_sm80.cu".
[4/48] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim192_bf16_sm80.o.d -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/cutlass/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/ben2/.venv/include -I/usr/include/python3.10 -c -c /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim192_bf16_sm80.cu -o /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim192_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1016"' -DTORCH_EXTENSION_NAME=flash_exp_2_cuda -D_GLIBCXX_USE_CXX11_ABI=1
FAILED: /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim192_bf16_sm80.o 
/usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim192_bf16_sm80.o.d -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/cutlass/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/ben2/.venv/include -I/usr/include/python3.10 -c -c /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim192_bf16_sm80.cu -o /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim192_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1016"' -DTORCH_EXTENSION_NAME=flash_exp_2_cuda -D_GLIBCXX_USE_CXX11_ABI=1
/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 265 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim192<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim192_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 265 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim192<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim192_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 265 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim192<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim192_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 265 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim192<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim192_bf16_sm80.cu

4 errors detected in the compilation of "/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim192_bf16_sm80.cu".
/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 265 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim192<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim192_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 265 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim192<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim192_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 265 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim192<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim192_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<192, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<192, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 265 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim192<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim192_bf16_sm80.cu

4 errors detected in the compilation of "/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim192_bf16_sm80.cu".
[5/48] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_fwd_split_hdim96_bf16_sm80.o.d -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/cutlass/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/ben2/.venv/include -I/usr/include/python3.10 -c -c /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_fwd_split_hdim96_bf16_sm80.cu -o /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_fwd_split_hdim96_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1016"' -DTORCH_EXTENSION_NAME=flash_exp_2_cuda -D_GLIBCXX_USE_CXX11_ABI=1
[6/48] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim256_bf16_sm80.o.d -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/cutlass/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/ben2/.venv/include -I/usr/include/python3.10 -c -c /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim256_bf16_sm80.cu -o /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim256_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1016"' -DTORCH_EXTENSION_NAME=flash_exp_2_cuda -D_GLIBCXX_USE_CXX11_ABI=1
FAILED: /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim256_bf16_sm80.o 
/usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim256_bf16_sm80.o.d -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/cutlass/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/ben2/.venv/include -I/usr/include/python3.10 -c -c /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim256_bf16_sm80.cu -o /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim256_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1016"' -DTORCH_EXTENSION_NAME=flash_exp_2_cuda -D_GLIBCXX_USE_CXX11_ABI=1
/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 293 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim256<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim256_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 293 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim256<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim256_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 293 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim256<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim256_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 293 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim256<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim256_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_2>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_2>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 293 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim256<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim256_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_2>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_2>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 293 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim256<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim256_bf16_sm80.cu

6 errors detected in the compilation of "/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim256_bf16_sm80.cu".
/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 293 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim256<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim256_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 293 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim256<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim256_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 293 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim256<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim256_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 293 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim256<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim256_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_2>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_2>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 293 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim256<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim256_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_2>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_2>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::bfloat16_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 293 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim256<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim256_bf16_sm80.cu

6 errors detected in the compilation of "/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim256_bf16_sm80.cu".
[7/48] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_fwd_split_hdim128_bf16_sm80.o.d -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/cutlass/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/ben2/.venv/include -I/usr/include/python3.10 -c -c /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_fwd_split_hdim128_bf16_sm80.cu -o /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_fwd_split_hdim128_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1016"' -DTORCH_EXTENSION_NAME=flash_exp_2_cuda -D_GLIBCXX_USE_CXX11_ABI=1
[8/48] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim96_fp16_sm80.o.d -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/cutlass/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/ben2/.venv/include -I/usr/include/python3.10 -c -c /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim96_fp16_sm80.cu -o /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim96_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1016"' -DTORCH_EXTENSION_NAME=flash_exp_2_cuda -D_GLIBCXX_USE_CXX11_ABI=1
FAILED: /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim96_fp16_sm80.o 
/usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim96_fp16_sm80.o.d -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/cutlass/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/ben2/.venv/include -I/usr/include/python3.10 -c -c /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim96_fp16_sm80.cu -o /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim96_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1016"' -DTORCH_EXTENSION_NAME=flash_exp_2_cuda -D_GLIBCXX_USE_CXX11_ABI=1
/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::_2, cute::_4>, cute::tuple<cute::_1, cute::_8>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::_2, cute::_4>, cute::tuple<cute::_1, cute::_8>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=true]" at line 186 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim96<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim96_fp16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::_2, cute::_4>, cute::tuple<cute::_1, cute::_8>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::_2, cute::_4>, cute::tuple<cute::_1, cute::_8>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=true]" at line 186 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim96<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim96_fp16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::_2, cute::_4>, cute::tuple<cute::_1, cute::_8>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::_2, cute::_4>, cute::tuple<cute::_1, cute::_8>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, true, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, true, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, true, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, true, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, true, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=false]" at line 186 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim96<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim96_fp16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::_2, cute::_4>, cute::tuple<cute::_1, cute::_8>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::_2, cute::_4>, cute::tuple<cute::_1, cute::_8>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, true, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, true, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, true, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, true, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, true, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=false]" at line 186 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim96<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim96_fp16_sm80.cu

4 errors detected in the compilation of "/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim96_fp16_sm80.cu".
/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::_2, cute::_4>, cute::tuple<cute::_1, cute::_8>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::_2, cute::_4>, cute::tuple<cute::_1, cute::_8>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=true]" at line 186 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim96<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim96_fp16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::_2, cute::_4>, cute::tuple<cute::_1, cute::_8>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::_2, cute::_4>, cute::tuple<cute::_1, cute::_8>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=true]" at line 186 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim96<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim96_fp16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::_2, cute::_4>, cute::tuple<cute::_1, cute::_8>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::_2, cute::_4>, cute::tuple<cute::_1, cute::_8>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, true, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, true, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, true, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, true, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, true, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=false]" at line 186 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim96<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim96_fp16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::_2, cute::_4>, cute::tuple<cute::_1, cute::_8>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::_2, cute::_4>, cute::tuple<cute::_1, cute::_8>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, true, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, true, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, true, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, true, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<96, 64, 128, 8, 2, 4, 4, true, false, cutlass::half_t, Flash_kernel_traits<96, 64, 128, 8, cutlass::half_t>>, Is_dropout=false]" at line 186 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim96<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim96_fp16_sm80.cu

4 errors detected in the compilation of "/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim96_fp16_sm80.cu".
[9/48] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim128_bf16_sm80.o.d -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/cutlass/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/ben2/.venv/include -I/usr/include/python3.10 -c -c /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim128_bf16_sm80.cu -o /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim128_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1016"' -DTORCH_EXTENSION_NAME=flash_exp_2_cuda -D_GLIBCXX_USE_CXX11_ABI=1
FAILED: /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim128_bf16_sm80.o 
/usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim128_bf16_sm80.o.d -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/cutlass/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/ben2/.venv/include -I/usr/include/python3.10 -c -c /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim128_bf16_sm80.cu -o /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim128_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1016"' -DTORCH_EXTENSION_NAME=flash_exp_2_cuda -D_GLIBCXX_USE_CXX11_ABI=1
/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::_2, cute::_4>, cute::tuple<cute::_1, cute::_8>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::_2, cute::_4>, cute::tuple<cute::_1, cute::_8>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 212 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim128<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim128_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::_2, cute::_4>, cute::tuple<cute::_1, cute::_8>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::_2, cute::_4>, cute::tuple<cute::_1, cute::_8>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 212 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim128<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim128_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 212 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim128<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim128_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 212 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim128<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim128_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::_2, cute::_4>, cute::tuple<cute::_1, cute::_8>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::_2, cute::_4>, cute::tuple<cute::_1, cute::_8>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 212 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim128<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim128_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::_2, cute::_4>, cute::tuple<cute::_1, cute::_8>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::_2, cute::_4>, cute::tuple<cute::_1, cute::_8>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 212 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim128<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim128_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 212 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim128<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim128_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 212 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim128<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim128_bf16_sm80.cu

8 errors detected in the compilation of "/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim128_bf16_sm80.cu".
/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::_2, cute::_4>, cute::tuple<cute::_1, cute::_8>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::_2, cute::_4>, cute::tuple<cute::_1, cute::_8>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 212 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim128<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim128_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::_2, cute::_4>, cute::tuple<cute::_1, cute::_8>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::_2, cute::_4>, cute::tuple<cute::_1, cute::_8>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 212 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim128<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim128_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 212 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim128<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim128_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 212 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim128<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim128_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::_2, cute::_4>, cute::tuple<cute::_1, cute::_8>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::_2, cute::_4>, cute::tuple<cute::_1, cute::_8>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 212 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim128<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim128_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::_2, cute::_4>, cute::tuple<cute::_1, cute::_8>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::_2, cute::_2>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::_2, cute::_4>, cute::tuple<cute::_1, cute::_8>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 212 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim128<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim128_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 212 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim128<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim128_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 64, 8, 4, 2, 2, true, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 212 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim128<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim128_bf16_sm80.cu

8 errors detected in the compilation of "/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim128_bf16_sm80.cu".
[10/48] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_fwd_split_hdim32_bf16_sm80.o.d -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/cutlass/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/ben2/.venv/include -I/usr/include/python3.10 -c -c /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_fwd_split_hdim32_bf16_sm80.cu -o /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_fwd_split_hdim32_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1016"' -DTORCH_EXTENSION_NAME=flash_exp_2_cuda -D_GLIBCXX_USE_CXX11_ABI=1
[11/48] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_fwd_split_hdim224_bf16_sm80.o.d -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/cutlass/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/ben2/.venv/include -I/usr/include/python3.10 -c -c /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_fwd_split_hdim224_bf16_sm80.cu -o /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_fwd_split_hdim224_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1016"' -DTORCH_EXTENSION_NAME=flash_exp_2_cuda -D_GLIBCXX_USE_CXX11_ABI=1
[12/48] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim160_bf16_sm80.o.d -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/cutlass/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/ben2/.venv/include -I/usr/include/python3.10 -c -c /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim160_bf16_sm80.cu -o /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim160_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1016"' -DTORCH_EXTENSION_NAME=flash_exp_2_cuda -D_GLIBCXX_USE_CXX11_ABI=1
FAILED: /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim160_bf16_sm80.o 
/usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim160_bf16_sm80.o.d -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/cutlass/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/ben2/.venv/include -I/usr/include/python3.10 -c -c /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim160_bf16_sm80.cu -o /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim160_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1016"' -DTORCH_EXTENSION_NAME=flash_exp_2_cuda -D_GLIBCXX_USE_CXX11_ABI=1
/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 245 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim160<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim160_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 245 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim160<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim160_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 245 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim160<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim160_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 245 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim160<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim160_bf16_sm80.cu

4 errors detected in the compilation of "/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim160_bf16_sm80.cu".
/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 245 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim160<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim160_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=true]" at line 245 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim160<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim160_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 245 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim160<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim160_bf16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<160, 64, 64, 8, 4, 4, 4, false, false, cutlass::bfloat16_t, Flash_kernel_traits<160, 64, 64, 8, cutlass::bfloat16_t>>, Is_dropout=false]" at line 245 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim160<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim160_bf16_sm80.cu

4 errors detected in the compilation of "/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim160_bf16_sm80.cu".
[13/48] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim224_fp16_sm80.o.d -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/cutlass/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/ben2/.venv/include -I/usr/include/python3.10 -c -c /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim224_fp16_sm80.cu -o /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim224_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1016"' -DTORCH_EXTENSION_NAME=flash_exp_2_cuda -D_GLIBCXX_USE_CXX11_ABI=1
FAILED: /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim224_fp16_sm80.o 
/usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim224_fp16_sm80.o.d -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/cutlass/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/ben2/.venv/include -I/usr/include/python3.10 -c -c /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim224_fp16_sm80.cu -o /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim224_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1016"' -DTORCH_EXTENSION_NAME=flash_exp_2_cuda -D_GLIBCXX_USE_CXX11_ABI=1
/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=true]" at line 277 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim224<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim224_fp16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=true]" at line 277 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim224<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim224_fp16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=false]" at line 277 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim224<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim224_fp16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=false]" at line 277 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim224<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim224_fp16_sm80.cu

4 errors detected in the compilation of "/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim224_fp16_sm80.cu".
/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=true]" at line 277 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim224<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim224_fp16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=true]" at line 277 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim224<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim224_fp16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=false]" at line 277 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim224<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim224_fp16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<224, 64, 64, 8, 4, 4, 4, false, false, cutlass::half_t, Flash_kernel_traits<224, 64, 64, 8, cutlass::half_t>>, Is_dropout=false]" at line 277 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim224<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim224_fp16_sm80.cu

4 errors detected in the compilation of "/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim224_fp16_sm80.cu".
[14/48] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim256_fp16_sm80.o.d -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/cutlass/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/ben2/.venv/include -I/usr/include/python3.10 -c -c /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim256_fp16_sm80.cu -o /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim256_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1016"' -DTORCH_EXTENSION_NAME=flash_exp_2_cuda -D_GLIBCXX_USE_CXX11_ABI=1
FAILED: /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim256_fp16_sm80.o 
/usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim256_fp16_sm80.o.d -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/cutlass/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/ben2/.venv/include -I/usr/include/python3.10 -c -c /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim256_fp16_sm80.cu -o /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_bwd_hdim256_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1016"' -DTORCH_EXTENSION_NAME=flash_exp_2_cuda -D_GLIBCXX_USE_CXX11_ABI=1
/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=true]" at line 293 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim256<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim256_fp16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=true]" at line 293 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim256<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim256_fp16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=false]" at line 293 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim256<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim256_fp16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=false]" at line 293 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim256<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim256_fp16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_2>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_2>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::half_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::half_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::half_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::half_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::half_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::half_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::half_t>>, Is_dropout=false]" at line 293 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim256<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim256_fp16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_2>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_2>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::half_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::half_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::half_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::half_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::half_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::half_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::half_t>>, Is_dropout=false]" at line 293 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim256<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim256_fp16_sm80.cu

6 errors detected in the compilation of "/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim256_fp16_sm80.cu".
/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=true]" at line 293 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim256<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim256_fp16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=true, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=true, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=true]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=true]" at line 293 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim256<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim256_fp16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=false]" at line 293 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim256<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim256_fp16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_4>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 64, 8, 4, 2, 2, false, false, cutlass::half_t, Flash_kernel_traits<256, 64, 64, 8, cutlass::half_t>>, Is_dropout=false]" at line 293 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim256<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim256_fp16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(423): error: expression must be a modifiable lvalue
                  a = 0.0f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_2>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_2>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::half_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::half_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::half_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::half_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::half_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::half_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::half_t>>, Is_dropout=false]" at line 293 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim256<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim256_fp16_sm80.cu

/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/softmax.h(428): error: expression must be a modifiable lvalue
                  a = c + a * c * (1.f - c) * 1.702f;
                  ^
          detected during:
            instantiation of "void flash::apply_gelu_backprop<Is_dropout,Engine0,Layout0,Engine1,Layout1>(cute::Tensor<Engine0, Layout0> &, cute::Tensor<Engine1, Layout1> &, float) [with Is_dropout=false, Engine0=cute::ViewEngine<float *>, Layout0=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_2>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>, Engine1=cute::ViewEngine<float *>, Layout1=cute::Layout<cute::tuple<cute::tuple<cute::C<2>, cute::C<1>>, cute::tuple<cute::_2, cute::_2>>, cute::tuple<cute::tuple<cute::C<2>, cute::C<0>>, cute::tuple<cute::_1, cute::_4>>>]" at line 551 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::half_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]" at line 805 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_kernel.h
            instantiation of "void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::half_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Params=Flash_bwd_params]" at line 32 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::half_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::half_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true]" at line 67 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::half_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::half_t>>, Is_dropout=false]" at line 101 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_flash_bwd<Kernel_traits,Is_dropout>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<256, 64, 32, 8, 4, 1, 2, true, true, cutlass::half_t, Flash_kernel_traits<256, 64, 32, 8, cutlass::half_t>>, Is_dropout=false]" at line 293 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_launch_template.h
            instantiation of "void run_mha_bwd_hdim256<T>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::half_t]" at line 9 of /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim256_fp16_sm80.cu

6 errors detected in the compilation of "/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_bwd_hdim256_fp16_sm80.cu".
[15/48] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_fwd_split_hdim160_bf16_sm80.o.d -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/cutlass/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/ben2/.venv/include -I/usr/include/python3.10 -c -c /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_fwd_split_hdim160_bf16_sm80.cu -o /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_fwd_split_hdim160_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1016"' -DTORCH_EXTENSION_NAME=flash_exp_2_cuda -D_GLIBCXX_USE_CXX11_ABI=1
[16/48] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_fwd_split_hdim96_fp16_sm80.o.d -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src -I/home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/cutlass/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/ben2/.venv/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/ben2/.venv/include -I/usr/include/python3.10 -c -c /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/csrc/flash_sigmoid/src/flash_fwd_split_hdim96_fp16_sm80.cu -o /home/ubuntu/ben2/ml-exp-attention/flash_sigmoid/build/temp.linux-aarch64-3.10/csrc/flash_sigmoid/src/flash_fwd_split_hdim96_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1016"' -DTORCH_EXTENSION_NAME=flash_exp_2_cuda -D_GLIBCXX_USE_CXX11_ABI=1
