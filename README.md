# Exponential Attention

This repository is a quick and dirty modification of [Apple/SigmoidAttention](https://github.com/apple/ml-sigmoid-attention) to compute the exponential function instead of sigmoid. That parent repo is in turn a modification of [Dao-AILab/FlashAttention2](https://github.com/Dao-AILab/flash-attention).

See [FlashSigmoid](./flash_sigmoid) for documentation on how to use this repo.

